{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "593619be-8090-4dad-a462-e883e560ec1c",
    "_uuid": "cba509cb-708d-4fd0-8e2a-e5c1c7a0982d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "import pandas\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ae9fd8d4-bb00-4b3d-8452-0ec605f6ebba",
    "_uuid": "add2f478-e634-42db-b3c6-732ebd484ce3"
   },
   "source": [
    "Detect TPUs or GPUs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "f5ccaf08-c532-4fde-9306-b897c890d0f8",
    "_uuid": "bc97f110-17eb-44dd-a792-d66c27a0b3a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on TPU  grpc://10.0.0.2:8470\n",
      "REPLICAS:  8\n"
     ]
    }
   ],
   "source": [
    "# Detect hardware, return appropriate distribution strategy\n",
    "try:\n",
    "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "    # set: this is always the case on Kaggle.\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c55830a8-39e7-4607-9691-cd0848a092f7",
    "_uuid": "b1188d07-d23d-4d04-b9f5-125d16b982ed"
   },
   "source": [
    "Set maximum sequence length and path variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "7acc953d-b1f6-4d29-a006-69af6839f7a9",
    "_uuid": "7d8d39c7-1fd2-4af7-a88a-9ca1242a6277"
   },
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 128\n",
    "\n",
    "# Note that private datasets cannot be copied - you'll have to share any pretrained models \n",
    "# you want to use with other competitors!\n",
    "GCS_PATH = KaggleDatasets().get_gcs_path('jigsaw-multilingual-toxic-comment-classification')\n",
    "BERT_GCS_PATH = KaggleDatasets().get_gcs_path('bert-multi')\n",
    "BERT_GCS_PATH_SAVEDMODEL = BERT_GCS_PATH + \"/bert_multi_from_tfhub\""
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b4c5c163-88f3-4021-9478-4005a360e7ce",
    "_uuid": "85bb06d9-cd83-41a8-a72f-e25c0b336d82"
   },
   "source": [
    "Define the model. We convert m-BERT's output to a final probabilty estimate. We're using an [m-BERT model from TensorFlow Hub](https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "fcd2093b-0774-4adf-9e4c-e9096f156d32",
    "_uuid": "1392a5e0-c8e4-46ea-b45d-0d9289682e09"
   },
   "outputs": [],
   "source": [
    "def multilingual_bert_model(max_seq_length=SEQUENCE_LENGTH, trainable_bert=True):\n",
    "    \"\"\"Build and return a multilingual BERT model and tokenizer.\"\"\"\n",
    "    input_word_ids = tf.keras.layers.Input(\n",
    "        shape=(max_seq_length,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = tf.keras.layers.Input(\n",
    "        shape=(max_seq_length,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = tf.keras.layers.Input(\n",
    "        shape=(max_seq_length,), dtype=tf.int32, name=\"all_segment_id\")\n",
    "    \n",
    "    # Load a SavedModel on TPU from GCS. This model is available online at \n",
    "    # https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/1. You can use your own \n",
    "    # pretrained models, but will need to add them as a Kaggle dataset.\n",
    "    bert_layer = tf.saved_model.load(BERT_GCS_PATH_SAVEDMODEL)\n",
    "    # Cast the loaded model to a TFHub KerasLayer.\n",
    "    bert_layer = hub.KerasLayer(bert_layer, trainable=trainable_bert)\n",
    "\n",
    "    pooled_output, outputs = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "    print(\"outputs.shape\", outputs.shape)\n",
    "    print(\"pooled_output.shape\", pooled_output.shape)\n",
    "    \n",
    "    #outputs = tf.keras.layers.GRU(128) (outputs)\n",
    "    #outputs = tf.keras.layers.Dropout(0.5) (outputs)\n",
    "    #print(\"outputs.shape after lstms\", outputs.shape)\n",
    "    #outputs = tf.keras.layers.Flatten() (outputs)\n",
    "    outputs = tf.keras.layers.GlobalMaxPooling1D() (outputs)\n",
    "    print(\"outputs.shape after flatten\", outputs.shape)\n",
    "    outputs = tf.keras.layers.Dense(32, activation='relu')(outputs)\n",
    "    \n",
    "    pooled_output = tf.keras.layers.Dense(32, activation='relu')(pooled_output)\n",
    "    \n",
    "    output = tf.keras.layers.Concatenate() ([pooled_output, outputs])\n",
    "    output = tf.keras.layers.Dense(1, activation='sigmoid', name='labels')(output)\n",
    "\n",
    "    return tf.keras.Model(inputs={'input_word_ids': input_word_ids,\n",
    "                                  'input_mask': input_mask,\n",
    "                                  'all_segment_id': segment_ids},\n",
    "                          outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "48479a32-25c1-40c9-bd1c-d076eb39d86e",
    "_uuid": "b24c47ad-a156-41f2-97b8-f5618181382c"
   },
   "source": [
    "Load the preprocessed dataset. See the demo notebook for sample code for performing this preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_string_list_into_ints(strlist):\n",
    "    s = tf.strings.strip(strlist)\n",
    "    s = tf.strings.substr(\n",
    "        strlist, 1, tf.strings.length(s) - 2)  # Remove parentheses around list\n",
    "    s = tf.strings.split(s, ',', maxsplit=SEQUENCE_LENGTH)\n",
    "    s = tf.strings.to_number(s, tf.int32)\n",
    "    s = tf.reshape(s, [SEQUENCE_LENGTH])  # Force shape here needed for XLA compilation (TPU)\n",
    "    return s\n",
    "\n",
    "def format_sentences(data, label='toxic', remove_language=False):\n",
    "    labels = {'labels': data.pop(label)}\n",
    "    if remove_language:\n",
    "        languages = {'language': data.pop('lang')}\n",
    "    # The remaining three items in the dict parsed from the CSV are lists of integers\n",
    "    for k,v in data.items():  # \"input_word_ids\", \"input_mask\", \"all_segment_id\"\n",
    "        data[k] = parse_string_list_into_ints(v)\n",
    "    return data, labels\n",
    "\n",
    "def make_sentence_dataset_from_csv(filename, label='toxic', language_to_filter=None):\n",
    "    # This assumes the column order label, input_word_ids, input_mask, segment_ids\n",
    "    SELECTED_COLUMNS = [label, \"input_word_ids\", \"input_mask\", \"all_segment_id\"]\n",
    "    label_default = tf.int32 if label == 'id' else tf.float32\n",
    "    COLUMN_DEFAULTS  = [label_default, tf.string, tf.string, tf.string]\n",
    "\n",
    "    if language_to_filter:\n",
    "        insert_pos = 0 if label != 'id' else 1\n",
    "        SELECTED_COLUMNS.insert(insert_pos, 'lang')\n",
    "        COLUMN_DEFAULTS.insert(insert_pos, tf.string)\n",
    "\n",
    "    preprocessed_sentences_dataset = tf.data.experimental.make_csv_dataset(\n",
    "        filename, column_defaults=COLUMN_DEFAULTS, select_columns=SELECTED_COLUMNS,\n",
    "        batch_size=1, num_epochs=1, shuffle=False)  # We'll do repeating and shuffling ourselves\n",
    "    # make_csv_dataset required a batch size, but we want to batch later\n",
    "    preprocessed_sentences_dataset = preprocessed_sentences_dataset.unbatch()\n",
    "    \n",
    "    if language_to_filter:\n",
    "        preprocessed_sentences_dataset = preprocessed_sentences_dataset.filter(\n",
    "            lambda data: tf.math.equal(data['lang'], tf.constant(language_to_filter)))\n",
    "        #preprocessed_sentences.pop('lang')\n",
    "    preprocessed_sentences_dataset = preprocessed_sentences_dataset.map(\n",
    "        lambda data: format_sentences(data, label=label,\n",
    "                                      remove_language=language_to_filter))\n",
    "\n",
    "    return preprocessed_sentences_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e006f30f-6350-45d5-b452-338c4bc78cc5",
    "_uuid": "10ff216a-2248-4104-858c-2b2461b42fba"
   },
   "source": [
    "Set up our data pipelines for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "b17b24d7-1b0f-442b-bfd2-c42748bcd067",
    "_uuid": "864f31ac-8285-442b-93cf-dcae11d7fe62"
   },
   "outputs": [],
   "source": [
    "def make_dataset_pipeline(dataset, repeat_and_shuffle=True):\n",
    "    \"\"\"Set up the pipeline for the given dataset.\n",
    "    \n",
    "    Caches, repeats, shuffles, and sets the pipeline up to prefetch batches.\"\"\"\n",
    "    cached_dataset = dataset.cache()\n",
    "    if repeat_and_shuffle:\n",
    "        cached_dataset = cached_dataset.repeat().shuffle(2048)\n",
    "    cached_dataset = cached_dataset.batch(32 * strategy.num_replicas_in_sync)\n",
    "    cached_dataset = cached_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return cached_dataset\n",
    "\n",
    "# Load the preprocessed English dataframe.\n",
    "preprocessed_en_filename = (\n",
    "    GCS_PATH + \"/jigsaw-toxic-comment-train-processed-seqlen{}.csv\".format(\n",
    "        SEQUENCE_LENGTH))\n",
    "\n",
    "# Set up the dataset and pipeline.\n",
    "english_train_dataset = make_dataset_pipeline(\n",
    "    make_sentence_dataset_from_csv(preprocessed_en_filename))\n",
    "\n",
    "# Process the new datasets by language.\n",
    "preprocessed_val_filename = (\n",
    "    GCS_PATH + \"/validation-processed-seqlen{}.csv\".format(SEQUENCE_LENGTH))\n",
    "\n",
    "nonenglish_val_datasets = {}\n",
    "for language_name, language_label in [('Spanish', 'es'), ('Italian', 'it'),\n",
    "                                      ('Turkish', 'tr')]:\n",
    "    nonenglish_val_datasets[language_name] = make_sentence_dataset_from_csv(\n",
    "        preprocessed_val_filename, language_to_filter=language_label)\n",
    "    nonenglish_val_datasets[language_name] = make_dataset_pipeline(\n",
    "        nonenglish_val_datasets[language_name])\n",
    "\n",
    "nonenglish_val_datasets['Combined'] = tf.data.experimental.sample_from_datasets(\n",
    "        (nonenglish_val_datasets['Spanish'], nonenglish_val_datasets['Italian'],\n",
    "         nonenglish_val_datasets['Turkish']))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ffd5c9ef-a806-4ae6-a1c5-8723ed822232",
    "_uuid": "0779be08-0502-47c0-b284-5f3d851de2e1"
   },
   "source": [
    "Compile our model. We'll first evaluate it on our new toxicity dataset in the \n",
    "different languages to see its performance. After that, we'll train it on one of our English datasets, and then again evaluate its performance on the new multilingual toxicity data. As our metric, we'll use the [AUC](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/AUC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "422a984e-e571-4898-9667-b95d38416ddd",
    "_uuid": "e3d569ca-0bf4-4bde-aa69-95a46908f65a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs.shape (None, None, 768)\n",
      "pooled_output.shape (None, 768)\n",
      "outputs.shape after flatten (None, 768)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "all_segment_id (InputLayer)     [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        [(None, 768), (None, 177853441   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 all_segment_id[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 768)          0           keras_layer[0][1]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 32)           24608       keras_layer[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 32)           24608       global_max_pooling1d[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 64)           0           dense_1[0][0]                    \n",
      "                                                                 dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "labels (Dense)                  (None, 1)            65          concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 177,902,722\n",
      "Trainable params: 177,902,721\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    multilingual_bert = multilingual_bert_model()\n",
    "\n",
    "    # Compile the model. Optimize using stochastic gradient descent.\n",
    "    multilingual_bert.compile(\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "        optimizer=tf.keras.optimizers.Adam(lr=1e-5),\n",
    "        metrics=[tf.keras.metrics.AUC()])\n",
    "\n",
    "multilingual_bert.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "f477d7f4-20ec-4858-87d6-c041313ad276",
    "_uuid": "730e2b4b-6e7c-43f3-912e-3cbea3db62c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train for 500.0 steps, validate for 100 steps\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 147s 294ms/step - loss: 0.1298 - auc: 0.9608 - val_loss: 0.4459 - val_auc: 0.8537\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 70s 141ms/step - loss: 0.1084 - auc: 0.9745 - val_loss: 0.4504 - val_auc: 0.8595\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 70s 140ms/step - loss: 0.0804 - auc: 0.9847 - val_loss: 0.5898 - val_auc: 0.8077\n",
      "Epoch 00003: early stopping\n",
      "Train for 500.0 steps\n",
      "Epoch 1/10\n",
      "500/500 [==============================] - 98s 196ms/step - loss: 0.0942 - auc: 0.9882\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 66s 132ms/step - loss: 0.0055 - auc: 0.9997\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 66s 132ms/step - loss: 0.0026 - auc: 0.9998\n",
      "Epoch 4/10\n",
      "500/500 [==============================] - 65s 131ms/step - loss: 0.0018 - auc: 0.9999\n",
      "Epoch 5/10\n",
      "500/500 [==============================] - 66s 132ms/step - loss: 0.0019 - auc: 0.9999\n",
      "Epoch 00005: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Test the model's performance on non-English comments before training.\n",
    "#for language in nonenglish_val_datasets:\n",
    "#    results = multilingual_bert.evaluate(nonenglish_val_datasets[language],\n",
    " #                                        steps=100, verbose=0)\n",
    "  #  print('{} loss, AUC before training:'.format(language), results)\n",
    "\n",
    "#results = multilingual_bert.evaluate(english_train_dataset,\n",
    "    #                                 steps=100, verbose=0)\n",
    "#print('\\nEnglish loss, AUC before training:', results)\n",
    "\n",
    "print()\n",
    "from keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor='val_auc', mode='max', verbose=1, patience=1)\n",
    "\n",
    "# Train on English Wikipedia comment data.\n",
    "history = multilingual_bert.fit(\n",
    "    # Set steps such that the number of examples per epoch is fixed.\n",
    "    # This makes training on different accelerators more comparable.\n",
    "    english_train_dataset, steps_per_epoch=4000/strategy.num_replicas_in_sync, #4000\n",
    "    epochs=10, verbose=1, validation_data=nonenglish_val_datasets['Combined'],\n",
    "    validation_steps=100, callbacks=[es])   #100\n",
    "\n",
    "# Re-evaluate the model's performance on non-English comments after training.\n",
    "#for language in nonenglish_val_datasets:\n",
    " #   results = multilingual_bert.evaluate(nonenglish_val_datasets[language],\n",
    " #                                        steps=100, verbose=0)\n",
    "  #  print('{} loss, AUC after training:'.format(language), results)\n",
    "\n",
    "#results = multilingual_bert.evaluate(english_train_dataset,\n",
    "  #                                   steps=100, verbose=0)\n",
    "#print('\\nEnglish loss, AUC after training:', results)\n",
    "\n",
    "# trian on validation data too\n",
    "es = EarlyStopping(monitor='auc', mode='max', verbose=1, patience=1)\n",
    "\n",
    "# Train on English Wikipedia comment data.\n",
    "history2 = multilingual_bert.fit(\n",
    "    nonenglish_val_datasets['Combined'], steps_per_epoch=4000/strategy.num_replicas_in_sync, #4000\n",
    "    epochs=10, verbose=1, callbacks=[es])   #100"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate predictions\n",
    "\n",
    "Finally, we'll use our trained multilingual model to generate predictions for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making dataset...\n",
      "Computing predictions...\n",
      "[3.3676624e-06 4.4703484e-07 9.9824965e-01 ... 2.7447939e-05 5.6624413e-07\n",
      " 5.0663948e-07]\n",
      "Generating submission file...\n",
      "id,toxic\r\n",
      "0,0.000003\r\n",
      "1,0.000000\r\n",
      "2,0.998250\r\n",
      "3,0.000001\r\n",
      "4,0.000000\r\n",
      "5,0.000000\r\n",
      "6,0.000001\r\n",
      "7,0.000001\r\n",
      "8,0.000003\r\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "TEST_DATASET_SIZE = 63812\n",
    "\n",
    "print('Making dataset...')\n",
    "preprocessed_test_filename = (\n",
    "    GCS_PATH + \"/test-processed-seqlen{}.csv\".format(SEQUENCE_LENGTH))\n",
    "test_dataset = make_sentence_dataset_from_csv(preprocessed_test_filename, label='id')\n",
    "test_dataset = make_dataset_pipeline(test_dataset, repeat_and_shuffle=False)\n",
    "\n",
    "print('Computing predictions...')\n",
    "test_sentences_dataset = test_dataset.map(lambda sentence, idnum: sentence)\n",
    "probabilities = np.squeeze(multilingual_bert.predict(test_sentences_dataset))\n",
    "print(probabilities)\n",
    "\n",
    "print('Generating submission file...')\n",
    "test_ids_dataset = test_dataset.map(lambda sentence, idnum: idnum).unbatch()\n",
    "test_ids = next(iter(test_ids_dataset.batch(TEST_DATASET_SIZE)))[\n",
    "    'labels'].numpy().astype('U')  # All in one batch\n",
    "\n",
    "np.savetxt('submission.csv', np.rec.fromarrays([test_ids, probabilities]),\n",
    "           fmt=['%s', '%f'], delimiter=',', header='id,toxic', comments='')\n",
    "!head submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
