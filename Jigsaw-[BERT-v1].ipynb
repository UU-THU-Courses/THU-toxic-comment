{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jigsaw Multilingual Toxic Comment Classification\n",
    "A notebook to bring all the packages under one roof. A collection of all modules implemented so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Implementation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'kaggle_datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-af2c0bae8b54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkaggle_datasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKaggleDatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'kaggle_datasets'"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------------------------------------------------#\n",
    "#                                                                                               #\n",
    "#   I M P O R T     L I B R A R I E S                                                           #\n",
    "#                                                                                               #\n",
    "#-----------------------------------------------------------------------------------------------#\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "import transformers\n",
    "\n",
    "from tokenizers import BertWordPieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect hardware, return appropriate distribution strategy\n",
    "try:\n",
    "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "    # set: this is always the case on Kaggle.\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#***********************************************************************************************#\n",
    "#                                                                                               #\n",
    "#   description:                                                                                #\n",
    "#   create a tokenizer to create tokens of the text input.                                      #\n",
    "#                                                                                               #\n",
    "#***********************************************************************************************#\n",
    "def tokenizer():\n",
    "    # First load the real tokenizer\n",
    "    tokenizer = transformers.DistilBertTokenizer.from_pretrained(\"distilbert-base-multilingual-cased\")\n",
    "    # Save the loaded tokenizer locally\n",
    "    tokenizer.save_pretrained('.')\n",
    "    # Reload it with the huggingface tokenizers library\n",
    "    fast_tokenizer = BertWordPieceTokenizer(\"vocab.txt\", lowercase=False)\n",
    "    # return the newly created tokenizer\n",
    "    return tokenizer, fast_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#***********************************************************************************************#\n",
    "#                                                                                               #\n",
    "#   description:                                                                                #\n",
    "#   create a encoder for batch data of the competition.                                         #\n",
    "#                                                                                               #\n",
    "#***********************************************************************************************#\n",
    "def data_encode(texts, tokenizer, chunk_size=256, maxlen=512):\n",
    "    \"\"\"\n",
    "    Encoder for encoding the text into sequence of integers for BERT Input\n",
    "    \"\"\"\n",
    "    tokenizer.enable_truncation(max_length=maxlen)\n",
    "    tokenizer.enable_padding(max_length=maxlen)\n",
    "    all_ids = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), chunk_size)):\n",
    "        text_chunk = texts[i:i+chunk_size].tolist()\n",
    "        encs = tokenizer.encode_batch(text_chunk)\n",
    "        all_ids.extend([enc.ids for enc in encs])\n",
    "    \n",
    "    return np.array(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#***********************************************************************************************#\n",
    "#                                                                                               #\n",
    "#   description:                                                                                #\n",
    "#   create a module for any sort of data pre-processing required.                               #\n",
    "#                                                                                               #\n",
    "#***********************************************************************************************#\n",
    "def preprocess(train1, valid, test, maxlen):\n",
    "    # create an instant of the tokenizer\n",
    "    _ , fast_tokenizer = tokenizer()\n",
    "    # pre-process the data by tokenizing and then encoding it\n",
    "    x_train = data_encode(train1.comment_text.astype(str), fast_tokenizer, maxlen=maxlen)\n",
    "    x_valid = data_encode(valid.comment_text.astype(str), fast_tokenizer, maxlen=maxlen)\n",
    "    x_test = data_encode(test.content.astype(str), fast_tokenizer, maxlen=maxlen)\n",
    "\n",
    "    y_train = train1.toxic.values\n",
    "    y_valid = valid.toxic.values\n",
    "        \n",
    "    return x_train, x_valid, x_test, y_train, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#***********************************************************************************************#\n",
    "#                                                                                               #\n",
    "#   description:                                                                                #\n",
    "#   create a dataloader to load the toxic comment classification data.                          #\n",
    "#                                                                                               #\n",
    "#***********************************************************************************************#\n",
    "def dataLoad(BATCH_SIZE=32, MAX_LEN=192):\n",
    "    \n",
    "    # configuration variable\n",
    "    AUTO = tf.data.experimental.AUTOTUNE\n",
    "    \n",
    "    # Dataset path\n",
    "    path=\"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/\"\n",
    "    \n",
    "    # LOADING THE DATA\n",
    "    train1 = pd.read_csv(path+\"jigsaw-toxic-comment-train.csv\")\n",
    "    valid = pd.read_csv(path+\"validation.csv\")\n",
    "    test = pd.read_csv(path+\"test.csv\")\n",
    "    \n",
    "    # apply the pre-processing step ont he loaded dataset\n",
    "    x_train, x_valid, x_test, y_train, y_valid = preprocess(train1, valid, test, MAX_LEN)\n",
    "    \n",
    "    # get data sizes\n",
    "    tr_size = x_train.shape[0]\n",
    "    vl_size = x_valid.shape[0]\n",
    "    \n",
    "    # Training dataset\n",
    "    train_dataset = (\n",
    "        tf.data.Dataset\n",
    "        .from_tensor_slices((x_train, y_train))\n",
    "        .repeat()\n",
    "        .shuffle(2048)\n",
    "        .batch(BATCH_SIZE)\n",
    "        .prefetch(AUTO)\n",
    "    )\n",
    "    \n",
    "    # Validation dataset\n",
    "    valid_dataset = (\n",
    "        tf.data.Dataset\n",
    "        .from_tensor_slices((x_valid, y_valid))\n",
    "        .batch(BATCH_SIZE)\n",
    "        .cache()\n",
    "        .prefetch(AUTO)\n",
    "    )\n",
    "\n",
    "    # Testing dataset\n",
    "    test_dataset = (\n",
    "        tf.data.Dataset\n",
    "        .from_tensor_slices(x_test)\n",
    "        .batch(BATCH_SIZE)\n",
    "    )\n",
    "    \n",
    "    # Return the newly created datsets\n",
    "    return train_dataset, valid_dataset, test_dataset, tr_size, vl_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#***********************************************************************************************#\n",
    "#                                                                                               #\n",
    "#   description:                                                                                #\n",
    "#   create a BERT model.                                                                        #\n",
    "#                                                                                               #\n",
    "#***********************************************************************************************#\n",
    "def build_model(transformer, max_len=512):\n",
    "    \"\"\"\n",
    "    Function for creating the BERT-1 model\n",
    "    \"\"\"\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    sequence_output = transformer(input_word_ids)[0]\n",
    "    cls_token = sequence_output[:, 0, :]\n",
    "    out = Dense(1, activation='sigmoid')(cls_token)\n",
    "    \n",
    "    model = Model(inputs=input_word_ids, outputs=out)\n",
    "    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
    "MAX_LEN = 192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with strategy.scope():\n",
    "    # Create an instance of the model\n",
    "    transformer_layer = (\n",
    "        transformers.TFDistilBertModel\n",
    "        .from_pretrained('distilbert-base-multilingual-cased')\n",
    "    )\n",
    "\n",
    "    model = build_model(transformer_layer, max_len=MAX_LEN)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data by calling the appropriate functions\n",
    "train_dataset, valid_dataset, test_dataset, tr_size, vl_size = dataLoad(BATCH_SIZE=BATCH_SIZE, MAX_LEN=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = tr_size // BATCH_SIZE\n",
    "train_history = model.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=n_steps,\n",
    "    validation_data=valid_dataset,\n",
    "    epochs=EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = vl_size // BATCH_SIZE\n",
    "train_history_2 = model.fit(\n",
    "    valid_dataset.repeat(),\n",
    "    steps_per_epoch=n_steps,\n",
    "    epochs=EPOCHS*2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a submission file with final predictions\n",
    "sub = pd.read_csv(\"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv\")\n",
    "sub['toxic'] = model.predict(test_dataset, verbose=1)\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
